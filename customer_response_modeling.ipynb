{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Goal of the project**","metadata":{}},{"cell_type":"markdown","source":"The predictive response models used to help identify customers in marketing can also be used to help outbound sales teams improve their call conversion rate by targeting the best people or companies to call. Whether we’re sending emails or using catalogue marketing, or calling customers by phone, the principles are identical - we’re aiming to increase profit by generating the maximum amount of revenue from the minimum amount of effort and cost.\n\nIn this project, I’ll create a customer response model. It uses [IBM Watson Analytics data](https://www.kaggle.com/datasets/pankajjsh06/ibm-watson-marketing-customer-value-data) to predict if a customer will accept a renewal proposal when called.","metadata":{}},{"cell_type":"markdown","source":"**Load the packages**","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.377124Z","iopub.execute_input":"2022-10-30T18:43:06.377553Z","iopub.status.idle":"2022-10-30T18:43:06.382607Z","shell.execute_reply.started":"2022-10-30T18:43:06.377522Z","shell.execute_reply":"2022-10-30T18:43:06.381486Z"},"trusted":true},"execution_count":274,"outputs":[]},{"cell_type":"markdown","source":"**Load the data**","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('../input/ibm-watson-marketing-customer-value-data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv').iloc[: , 1:]","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.387367Z","iopub.execute_input":"2022-10-30T18:43:06.387956Z","iopub.status.idle":"2022-10-30T18:43:06.447861Z","shell.execute_reply.started":"2022-10-30T18:43:06.387924Z","shell.execute_reply":"2022-10-30T18:43:06.446715Z"},"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"# Rename Pandas columns to lower case\ndf.columns = df.columns.str.lower()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.449810Z","iopub.execute_input":"2022-10-30T18:43:06.450159Z","iopub.status.idle":"2022-10-30T18:43:06.455500Z","shell.execute_reply.started":"2022-10-30T18:43:06.450129Z","shell.execute_reply":"2022-10-30T18:43:06.454310Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"df = df.applymap(lambda s: s.lower() if type(s) == str else s)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.456694Z","iopub.execute_input":"2022-10-30T18:43:06.457013Z","iopub.status.idle":"2022-10-30T18:43:06.571327Z","shell.execute_reply.started":"2022-10-30T18:43:06.456956Z","shell.execute_reply":"2022-10-30T18:43:06.569874Z"},"trusted":true},"execution_count":277,"outputs":[]},{"cell_type":"code","source":"# Examine the data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.574230Z","iopub.execute_input":"2022-10-30T18:43:06.574655Z","iopub.status.idle":"2022-10-30T18:43:06.603810Z","shell.execute_reply.started":"2022-10-30T18:43:06.574615Z","shell.execute_reply":"2022-10-30T18:43:06.602929Z"},"trusted":true},"execution_count":278,"outputs":[{"execution_count":278,"output_type":"execute_result","data":{"text/plain":"        state  customer lifetime value response  coverage education  \\\n0  washington              2763.519279       no     basic  bachelor   \n1     arizona              6979.535903       no  extended  bachelor   \n2      nevada             12887.431650       no   premium  bachelor   \n3  california              7645.861827       no     basic  bachelor   \n4  washington              2813.692575       no     basic  bachelor   \n\n  effective to date employmentstatus gender  income location code  ...  \\\n0           2/24/11         employed      f   56274      suburban  ...   \n1           1/31/11       unemployed      f       0      suburban  ...   \n2           2/19/11         employed      f   48767      suburban  ...   \n3           1/20/11       unemployed      m       0      suburban  ...   \n4            2/3/11         employed      m   43836         rural  ...   \n\n  months since policy inception  number of open complaints  \\\n0                             5                          0   \n1                            42                          0   \n2                            38                          0   \n3                            65                          0   \n4                            44                          0   \n\n   number of policies     policy type        policy  renew offer type  \\\n0                   1  corporate auto  corporate l3            offer1   \n1                   8   personal auto   personal l3            offer3   \n2                   2   personal auto   personal l3            offer1   \n3                   7  corporate auto  corporate l2            offer1   \n4                   1   personal auto   personal l1            offer1   \n\n  sales channel total claim amount  vehicle class vehicle size  \n0         agent         384.811147   two-door car      medsize  \n1         agent        1131.464935  four-door car      medsize  \n2         agent         566.472247   two-door car      medsize  \n3   call center         529.881344            suv      medsize  \n4         agent         138.130879  four-door car      medsize  \n\n[5 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>customer lifetime value</th>\n      <th>response</th>\n      <th>coverage</th>\n      <th>education</th>\n      <th>effective to date</th>\n      <th>employmentstatus</th>\n      <th>gender</th>\n      <th>income</th>\n      <th>location code</th>\n      <th>...</th>\n      <th>months since policy inception</th>\n      <th>number of open complaints</th>\n      <th>number of policies</th>\n      <th>policy type</th>\n      <th>policy</th>\n      <th>renew offer type</th>\n      <th>sales channel</th>\n      <th>total claim amount</th>\n      <th>vehicle class</th>\n      <th>vehicle size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>washington</td>\n      <td>2763.519279</td>\n      <td>no</td>\n      <td>basic</td>\n      <td>bachelor</td>\n      <td>2/24/11</td>\n      <td>employed</td>\n      <td>f</td>\n      <td>56274</td>\n      <td>suburban</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>corporate auto</td>\n      <td>corporate l3</td>\n      <td>offer1</td>\n      <td>agent</td>\n      <td>384.811147</td>\n      <td>two-door car</td>\n      <td>medsize</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>arizona</td>\n      <td>6979.535903</td>\n      <td>no</td>\n      <td>extended</td>\n      <td>bachelor</td>\n      <td>1/31/11</td>\n      <td>unemployed</td>\n      <td>f</td>\n      <td>0</td>\n      <td>suburban</td>\n      <td>...</td>\n      <td>42</td>\n      <td>0</td>\n      <td>8</td>\n      <td>personal auto</td>\n      <td>personal l3</td>\n      <td>offer3</td>\n      <td>agent</td>\n      <td>1131.464935</td>\n      <td>four-door car</td>\n      <td>medsize</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nevada</td>\n      <td>12887.431650</td>\n      <td>no</td>\n      <td>premium</td>\n      <td>bachelor</td>\n      <td>2/19/11</td>\n      <td>employed</td>\n      <td>f</td>\n      <td>48767</td>\n      <td>suburban</td>\n      <td>...</td>\n      <td>38</td>\n      <td>0</td>\n      <td>2</td>\n      <td>personal auto</td>\n      <td>personal l3</td>\n      <td>offer1</td>\n      <td>agent</td>\n      <td>566.472247</td>\n      <td>two-door car</td>\n      <td>medsize</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>california</td>\n      <td>7645.861827</td>\n      <td>no</td>\n      <td>basic</td>\n      <td>bachelor</td>\n      <td>1/20/11</td>\n      <td>unemployed</td>\n      <td>m</td>\n      <td>0</td>\n      <td>suburban</td>\n      <td>...</td>\n      <td>65</td>\n      <td>0</td>\n      <td>7</td>\n      <td>corporate auto</td>\n      <td>corporate l2</td>\n      <td>offer1</td>\n      <td>call center</td>\n      <td>529.881344</td>\n      <td>suv</td>\n      <td>medsize</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>washington</td>\n      <td>2813.692575</td>\n      <td>no</td>\n      <td>basic</td>\n      <td>bachelor</td>\n      <td>2/3/11</td>\n      <td>employed</td>\n      <td>m</td>\n      <td>43836</td>\n      <td>rural</td>\n      <td>...</td>\n      <td>44</td>\n      <td>0</td>\n      <td>1</td>\n      <td>personal auto</td>\n      <td>personal l1</td>\n      <td>offer1</td>\n      <td>agent</td>\n      <td>138.130879</td>\n      <td>four-door car</td>\n      <td>medsize</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Overview of all variables, their datatypes\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.605383Z","iopub.execute_input":"2022-10-30T18:43:06.606241Z","iopub.status.idle":"2022-10-30T18:43:06.627601Z","shell.execute_reply.started":"2022-10-30T18:43:06.606204Z","shell.execute_reply":"2022-10-30T18:43:06.626413Z"},"trusted":true},"execution_count":279,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9134 entries, 0 to 9133\nData columns (total 23 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   state                          9134 non-null   object \n 1   customer lifetime value        9134 non-null   float64\n 2   response                       9134 non-null   object \n 3   coverage                       9134 non-null   object \n 4   education                      9134 non-null   object \n 5   effective to date              9134 non-null   object \n 6   employmentstatus               9134 non-null   object \n 7   gender                         9134 non-null   object \n 8   income                         9134 non-null   int64  \n 9   location code                  9134 non-null   object \n 10  marital status                 9134 non-null   object \n 11  monthly premium auto           9134 non-null   int64  \n 12  months since last claim        9134 non-null   int64  \n 13  months since policy inception  9134 non-null   int64  \n 14  number of open complaints      9134 non-null   int64  \n 15  number of policies             9134 non-null   int64  \n 16  policy type                    9134 non-null   object \n 17  policy                         9134 non-null   object \n 18  renew offer type               9134 non-null   object \n 19  sales channel                  9134 non-null   object \n 20  total claim amount             9134 non-null   float64\n 21  vehicle class                  9134 non-null   object \n 22  vehicle size                   9134 non-null   object \ndtypes: float64(2), int64(6), object(15)\nmemory usage: 1.6+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Define the target variable**","metadata":{}},{"cell_type":"markdown","source":"At the moment, the response column contains a boolean yes or no value, but we need to “binarise” this to turn it into a numeric value the model can use. A simple replace( ) is one of several ways to do this.","metadata":{}},{"cell_type":"code","source":"df['response'] = df['response'].replace(('yes', 'no'), (1, 0))","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.629146Z","iopub.execute_input":"2022-10-30T18:43:06.630330Z","iopub.status.idle":"2022-10-30T18:43:06.641250Z","shell.execute_reply.started":"2022-10-30T18:43:06.630294Z","shell.execute_reply":"2022-10-30T18:43:06.640180Z"},"trusted":true},"execution_count":280,"outputs":[]},{"cell_type":"markdown","source":"As we’ll see from examining the value_counts( ) of the target variable column, this dataset is imbalanced.","metadata":{}},{"cell_type":"code","source":"df['response'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.642432Z","iopub.execute_input":"2022-10-30T18:43:06.642747Z","iopub.status.idle":"2022-10-30T18:43:06.653628Z","shell.execute_reply.started":"2022-10-30T18:43:06.642718Z","shell.execute_reply":"2022-10-30T18:43:06.652404Z"},"trusted":true},"execution_count":281,"outputs":[{"execution_count":281,"output_type":"execute_result","data":{"text/plain":"0    7826\n1    1308\nName: response, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Changing a date string to a datetime**","metadata":{}},{"cell_type":"code","source":"df['effective to date'] = pd.to_datetime(df['effective to date'])","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.655245Z","iopub.execute_input":"2022-10-30T18:43:06.655546Z","iopub.status.idle":"2022-10-30T18:43:06.675020Z","shell.execute_reply.started":"2022-10-30T18:43:06.655520Z","shell.execute_reply":"2022-10-30T18:43:06.673720Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"markdown","source":"**Engineer datetime feature**","metadata":{}},{"cell_type":"markdown","source":"Basically we can break apart the effective to date and get the week.","metadata":{}},{"cell_type":"code","source":"df['week'] = df['effective to date'].dt.strftime('%W').astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.677024Z","iopub.execute_input":"2022-10-30T18:43:06.677463Z","iopub.status.idle":"2022-10-30T18:43:06.735531Z","shell.execute_reply.started":"2022-10-30T18:43:06.677420Z","shell.execute_reply":"2022-10-30T18:43:06.734162Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"df = df.drop('effective to date', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.739428Z","iopub.execute_input":"2022-10-30T18:43:06.740282Z","iopub.status.idle":"2022-10-30T18:43:06.751927Z","shell.execute_reply.started":"2022-10-30T18:43:06.740242Z","shell.execute_reply":"2022-10-30T18:43:06.751050Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"markdown","source":"**Check for missing values**","metadata":{}},{"cell_type":"markdown","source":"Before moving on, we’ll check to see if there are any null values to impute. However, the data were all fine, so there was nothing to do.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.754675Z","iopub.execute_input":"2022-10-30T18:43:06.755170Z","iopub.status.idle":"2022-10-30T18:43:06.773741Z","shell.execute_reply.started":"2022-10-30T18:43:06.755118Z","shell.execute_reply":"2022-10-30T18:43:06.772474Z"},"trusted":true},"execution_count":285,"outputs":[{"execution_count":285,"output_type":"execute_result","data":{"text/plain":"state                            0\ncustomer lifetime value          0\nresponse                         0\ncoverage                         0\neducation                        0\nemploymentstatus                 0\ngender                           0\nincome                           0\nlocation code                    0\nmarital status                   0\nmonthly premium auto             0\nmonths since last claim          0\nmonths since policy inception    0\nnumber of open complaints        0\nnumber of policies               0\npolicy type                      0\npolicy                           0\nrenew offer type                 0\nsales channel                    0\ntotal claim amount               0\nvehicle class                    0\nvehicle size                     0\nweek                             0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Define X and y**","metadata":{}},{"cell_type":"code","source":"X = df.drop('response', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.775396Z","iopub.execute_input":"2022-10-30T18:43:06.775916Z","iopub.status.idle":"2022-10-30T18:43:06.787462Z","shell.execute_reply.started":"2022-10-30T18:43:06.775874Z","shell.execute_reply":"2022-10-30T18:43:06.786086Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"code","source":"y = df['response']","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.788699Z","iopub.execute_input":"2022-10-30T18:43:06.789236Z","iopub.status.idle":"2022-10-30T18:43:06.797080Z","shell.execute_reply.started":"2022-10-30T18:43:06.789203Z","shell.execute_reply":"2022-10-30T18:43:06.795906Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"markdown","source":"**Create the training and test datasets**","metadata":{}},{"cell_type":"markdown","source":"To divide X and y into the train and test datasets we need to train the model we will use the train_test_split( ) function from scikit-learn. We’ll assign 30% of the data to the test groups using the argument test_size = 0.3, and we’ll use the stratify = y option to ensure the target variable is present in the test and train data in equal proportions.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.798422Z","iopub.execute_input":"2022-10-30T18:43:06.798764Z","iopub.status.idle":"2022-10-30T18:43:06.807858Z","shell.execute_reply.started":"2022-10-30T18:43:06.798734Z","shell.execute_reply":"2022-10-30T18:43:06.806921Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"# Isolate X and y variables, and perform train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.809479Z","iopub.execute_input":"2022-10-30T18:43:06.810034Z","iopub.status.idle":"2022-10-30T18:43:06.835483Z","shell.execute_reply.started":"2022-10-30T18:43:06.809987Z","shell.execute_reply":"2022-10-30T18:43:06.834445Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"markdown","source":"**Create a pipeline**","metadata":{}},{"cell_type":"markdown","source":"First, we’re extracting the numeric columns from X and returning a list, then we’re doing the same with the categorical data columns. For the categorical data, we’re applying the OneHotEncoder( ) to ensure everything is numeric. These settings get passed to the ColumnTransformer( ).\n\nNext, we’re using the pipeline from imblearn, which is specifically designed for working with imbalanced datasets like this one. The pipeline first runs our preprocessor we just created, then uses the SVM-SMOTE method to handle the class imbalance. Finally, we use the StandardScaler( ) to put our data on a consistent scale, then pass in the model and return a bundled_pipeline.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom imblearn.pipeline import Pipeline as imbpipeline\nfrom imblearn.over_sampling import SVMSMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.836935Z","iopub.execute_input":"2022-10-30T18:43:06.838198Z","iopub.status.idle":"2022-10-30T18:43:06.843829Z","shell.execute_reply.started":"2022-10-30T18:43:06.838158Z","shell.execute_reply":"2022-10-30T18:43:06.842840Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"def get_pipeline(X, model):\n\n    numeric_columns = list(X.select_dtypes(exclude = ['object']).columns.values.tolist())    \n    categorical_columns = list(X.select_dtypes(include = ['object']).columns.values.tolist())\n    categorical_pipeline = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n    \n    preprocessor = ColumnTransformer(transformers = [('numeric', 'passthrough', numeric_columns),\n                                                     ('categorical', categorical_pipeline, categorical_columns)], remainder = 'passthrough')\n\n    bundled_pipeline = imbpipeline(steps = [('preprocessor', preprocessor),\n                                            ('smote', SVMSMOTE(random_state = 42)),\n                                            ('scaler', StandardScaler()),\n                                            ('model', model)])\n    \n    return bundled_pipeline","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.845725Z","iopub.execute_input":"2022-10-30T18:43:06.846186Z","iopub.status.idle":"2022-10-30T18:43:06.856839Z","shell.execute_reply.started":"2022-10-30T18:43:06.846143Z","shell.execute_reply":"2022-10-30T18:43:06.855764Z"},"trusted":true},"execution_count":291,"outputs":[]},{"cell_type":"markdown","source":"**Select the best model for the job**","metadata":{}},{"cell_type":"markdown","source":"Rather than simply selecting a single model, or repeating our code manually on a range of models, we can create another function to automatically test a wide range of possible models to determine the best one for our needs.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier, XGBRFClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.858263Z","iopub.execute_input":"2022-10-30T18:43:06.858622Z","iopub.status.idle":"2022-10-30T18:43:06.873885Z","shell.execute_reply.started":"2022-10-30T18:43:06.858591Z","shell.execute_reply":"2022-10-30T18:43:06.873038Z"},"trusted":true},"execution_count":292,"outputs":[]},{"cell_type":"code","source":"def select_model(X, y, pipeline = None):\n\n  classifiers = {}\n  classifiers.update({'XGBClassifier': XGBClassifier(random_state = 42)})\n  classifiers.update({'XGBRFClassifier': XGBRFClassifier(random_state = 42)})\n  classifiers.update({'LGBMClassifier': LGBMClassifier(random_state = 42)})\n  classifiers.update({'DecisionTreeClassifier': DecisionTreeClassifier(random_state = 42)})\n  classifiers.update({'RandomForestClassifier': RandomForestClassifier(random_state = 42)})\n  classifiers.update({'ExtraTreesClassifier': ExtraTreesClassifier(random_state = 42)})\n  classifiers.update({'GradientBoostingClassifier': GradientBoostingClassifier(random_state = 42)})    \n  classifiers.update({'BaggingClassifier': BaggingClassifier(random_state = 42)})\n  classifiers.update({'AdaBoostClassifier': AdaBoostClassifier(random_state = 42)})\n  classifiers.update({'HistGradientBoostingClassifier': HistGradientBoostingClassifier(random_state = 42)})\n  classifiers.update({'CatBoostClassifier': CatBoostClassifier(silent = True, random_state = 42)})\n\n  df_models = pd.DataFrame(columns = ['model', 'run_time', 'f1_score_cv', 'f1_score'])\n\n  for key in classifiers:\n\n      print('*', key)\n\n      start_time = time.time()\n      \n      pipeline = get_pipeline(X_train, classifiers[key])\n\n      cv = cross_val_score(pipeline, X, y, cv = 5, scoring = 'f1_macro', n_jobs = -1)\n\n      pipeline.fit(X_train, y_train)\n      y_pred = pipeline.predict(X_test)\n\n      row = {'model': key,\n             'run_time': format(round((time.time() - start_time) / 60, 2)),\n             'f1_score_cv': cv.mean(),\n             'f1_score': f1_score(y_test, y_pred, average = 'macro')}\n\n      df_models = df_models.append(row, ignore_index = True)\n\n  df_models = df_models.sort_values(by = 'f1_score', ascending = False)\n      \n  return df_models","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.875230Z","iopub.execute_input":"2022-10-30T18:43:06.875812Z","iopub.status.idle":"2022-10-30T18:43:06.889024Z","shell.execute_reply.started":"2022-10-30T18:43:06.875779Z","shell.execute_reply":"2022-10-30T18:43:06.887898Z"},"trusted":true},"execution_count":293,"outputs":[]},{"cell_type":"code","source":"models = select_model(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:43:06.890621Z","iopub.execute_input":"2022-10-30T18:43:06.890959Z","iopub.status.idle":"2022-10-30T18:46:21.127503Z","shell.execute_reply.started":"2022-10-30T18:43:06.890928Z","shell.execute_reply":"2022-10-30T18:46:21.126362Z"},"trusted":true},"execution_count":294,"outputs":[{"name":"stdout","text":"* XGBClassifier\n* XGBRFClassifier\n* LGBMClassifier\n* DecisionTreeClassifier\n* RandomForestClassifier\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* BaggingClassifier\n* AdaBoostClassifier\n* HistGradientBoostingClassifier\n* CatBoostClassifier\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Running the select_model( ) function on our training data takes a minute or so. The best independent model was BaggingClassifier. ","metadata":{}},{"cell_type":"code","source":"models.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:46:21.131640Z","iopub.execute_input":"2022-10-30T18:46:21.132033Z","iopub.status.idle":"2022-10-30T18:46:21.146538Z","shell.execute_reply.started":"2022-10-30T18:46:21.131994Z","shell.execute_reply":"2022-10-30T18:46:21.145078Z"},"trusted":true},"execution_count":295,"outputs":[{"execution_count":295,"output_type":"execute_result","data":{"text/plain":"                             model run_time  f1_score_cv  f1_score\n10              CatBoostClassifier     1.51     0.967732  0.983010\n0                    XGBClassifier     0.32     0.970514  0.981013\n5             ExtraTreesClassifier     0.15     0.957702  0.979625\n4           RandomForestClassifier     0.16     0.947453  0.969381\n2                   LGBMClassifier     0.13     0.905936  0.928180\n7                BaggingClassifier     0.12     0.940206  0.925641\n9   HistGradientBoostingClassifier     0.14     0.909291  0.912114\n3           DecisionTreeClassifier     0.07     0.886083  0.846914\n6       GradientBoostingClassifier     0.29     0.649959  0.655731\n8               AdaBoostClassifier     0.12     0.624645  0.626591","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>run_time</th>\n      <th>f1_score_cv</th>\n      <th>f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>CatBoostClassifier</td>\n      <td>1.51</td>\n      <td>0.967732</td>\n      <td>0.983010</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGBClassifier</td>\n      <td>0.32</td>\n      <td>0.970514</td>\n      <td>0.981013</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ExtraTreesClassifier</td>\n      <td>0.15</td>\n      <td>0.957702</td>\n      <td>0.979625</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RandomForestClassifier</td>\n      <td>0.16</td>\n      <td>0.947453</td>\n      <td>0.969381</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LGBMClassifier</td>\n      <td>0.13</td>\n      <td>0.905936</td>\n      <td>0.928180</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BaggingClassifier</td>\n      <td>0.12</td>\n      <td>0.940206</td>\n      <td>0.925641</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>HistGradientBoostingClassifier</td>\n      <td>0.14</td>\n      <td>0.909291</td>\n      <td>0.912114</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier</td>\n      <td>0.07</td>\n      <td>0.886083</td>\n      <td>0.846914</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GradientBoostingClassifier</td>\n      <td>0.29</td>\n      <td>0.649959</td>\n      <td>0.655731</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AdaBoostClassifier</td>\n      <td>0.12</td>\n      <td>0.624645</td>\n      <td>0.626591</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Finally, we can take our best model and fit the data on this.","metadata":{}},{"cell_type":"code","source":"selected_model = BaggingClassifier(random_state = 42)\nbundled_pipeline = get_pipeline(X_train, selected_model)\nbundled_pipeline.fit(X_train, y_train)\ny_pred = bundled_pipeline.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:46:21.147710Z","iopub.execute_input":"2022-10-30T18:46:21.148103Z","iopub.status.idle":"2022-10-30T18:46:23.852153Z","shell.execute_reply.started":"2022-10-30T18:46:21.148070Z","shell.execute_reply":"2022-10-30T18:46:23.851214Z"},"trusted":true},"execution_count":296,"outputs":[]},{"cell_type":"markdown","source":"**Assess the performance of the model**","metadata":{}},{"cell_type":"markdown","source":"To examine how well the model performed in a little more detail we can make use of the classification_report( ) function. The classification report shows us the precision, recall, and F1 score for our predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:46:23.853314Z","iopub.execute_input":"2022-10-30T18:46:23.853618Z","iopub.status.idle":"2022-10-30T18:46:23.858782Z","shell.execute_reply.started":"2022-10-30T18:46:23.853590Z","shell.execute_reply":"2022-10-30T18:46:23.857542Z"},"trusted":true},"execution_count":297,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-10-30T18:46:23.860265Z","iopub.execute_input":"2022-10-30T18:46:23.860697Z","iopub.status.idle":"2022-10-30T18:46:23.879533Z","shell.execute_reply.started":"2022-10-30T18:46:23.860654Z","shell.execute_reply":"2022-10-30T18:46:23.878229Z"},"trusted":true},"execution_count":298,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      2348\n           1       0.90      0.85      0.87       393\n\n    accuracy                           0.96      2741\n   macro avg       0.94      0.92      0.93      2741\nweighted avg       0.96      0.96      0.96      2741\n\n","output_type":"stream"}]}]}